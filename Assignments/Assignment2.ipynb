{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from dmba import printTermDocumentMatrix, classificationSummary\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\gabri\\Downloads\\farm-ads.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-abdominal ad-aortic ad-aneurysm ad-doctorf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-abdominal ad-aortic ad-aneurysm ad-million...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-absorbent ad-oil ad-snar ad-factory ad-dir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-acid ad-reflux ad-relief ad-top ad-treatme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-acid ad-reflux ad-symptom ad-acid ad-reflu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label                                               Text\n",
       "0     -1   ad-abdominal ad-aortic ad-aneurysm ad-doctorf...\n",
       "1     -1   ad-abdominal ad-aortic ad-aneurysm ad-million...\n",
       "2     -1   ad-absorbent ad-oil ad-snar ad-factory ad-dir...\n",
       "3     -1   ad-acid ad-reflux ad-relief ad-top ad-treatme...\n",
       "4     -1   ad-acid ad-reflux ad-symptom ad-acid ad-reflu..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4143 entries, 0 to 4142\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Label   4143 non-null   int64 \n",
      " 1   Text    4143 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 64.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Some relevant ads:\n",
      " ad-cheese ad-mak ad-consultant ad-expert ad-start ad-finish ad-help ad-setup ad-cheese ad-facility title-cheese title-consultant title-cheese title-mak title-consultant title-cheese title-mak title-workshop title-cheese title-mak title-expert cheese consultant cheese mak consultant cheese mak workshop cheese mak expert helpful link neville mcnaughton brian civitello brian richter ann wilkinson jim gage customer video technical consult plant layout design build material spec custom equipment design sanitary air system gmp ssop haccp plant audit milk parlor audit train seminar site course public speak product evaluation grant write product vendor cheezsorce customer innovative cheese maker dairy processor north america knowledge technology product help customer product consumer table provide complete service program tak client idea concept reality business plan manufacture guide customer plant design process selection cheese mak equipment manufacture procedure provide customer aspect actual hand experience art mak cheesis info click practice produce result cheezsorce ship address mccausland ave st loue mo site create webcenter\n",
      " ad-raise ad-dairy ad-goat ad-guide ad-raise ad-dairy ad-goat ad-info ad-tip ad-advice ad-learn title-goat title-raise title-dairy title-goat header-goat goat raise dairy goat special offer close free subscription simply fill form below click subscribe ll send live country life magazine fill landscape garden recipe build family life idea country live absolutely free charge email name name address city zip day birth total acreage size close home customer service help home email newsletter photo gallery product guide community video radio live country life animal pet livestock comment print share goat goat serve friendly pet provide source income lucrative alternative cattle sheep page story raise dairy goat try farmer market mak artisan goat cheese start raise dairy goat goat serve friendly pet provide source income lucrative alternative cattle sheep actually surprise goat milk depend breed age goat produce abundant amount milk sold local farmer market dairy goat owner liza plaster animal north carolina farm buy animal run goat dairy fence choose fence protect neighbor property goat protect goat shouldn eate fence investment goat love rocky hill mind decide fence continue page try farmer market free subscription simply fill form below click subscribe ll send live country life magazine fill landscape garden recipe build family life idea country live absolutely free charge name name address city zip code email address day birth total acreage size comment comment rcdualrate wrote goat owner look fist starter goat live illinoe thank pm report abuse anonymous wrote goat mommy son fix continue try breed help doe fix apart alot notice sh crazy pm report abuse prev add comment log leave comment register log country home beautiful acreage improvement green machinery tractor mower tractor attachment power equipment trailer utility vehicle atv build barn gazebo sh fence property plan real estate garden beautiful garden flower vegetable landscap tree pond beautiful pond pond management tool animal dog horse exotic animal poultry livestock wildlife habitat bird pest control lifestyle food recipe recreation preview issue renew subscription free newsletter change address update profile past issue visit cla ad google york coupon ridiculously huge coupon day nyc www groupon com york backyard ur dream build waterfall pond deck outdoor bbq lighte www rocksolidyard com et plus welcome et plus generation tv etonline com etplus queen landscape garden enjoy affordable pric meet garden supply www keilbro com bathroom remodel quote top rate bathroom pro free bid www servicemagic com live country life beautiful solution product guide community video tv radio forums share photo email newsletter join log privacy policy children privacy policy visitor agreement discussion policy write user support media kit advertise magazine subscribe free renew subscription update account advertise magazine copyright\n",
      " ad-equine ad-supplement ad-buy ad-nutritional ad-supplement ad-horse ad-wellness ad-performance title-equine title-platinumperformance header-equine equine platinumperformance horse dog cat product platinum performance platinum performance cj ortho chon ortho chon ii hemo flo bio sponge skin allergy formula platinum potency category total horse health athletic performance bone density hoof support immune support digestive health joint support metabolic support reproduction fertility skin allergy weight management product platinum pak guide pak pak meet platinum horse platinum platinum platinum advisor platinum partner sponsorship contact log view cart foundation platinum bar athletic performance bone density cardio circulatory support immune support joint support vitamin mineral weight loss product horse total horse health athletic performance bone health hoof support immune health digestive health joint support metabolic support reproduction fertility skin allergy weight management product dog cat foundation athletic performance bone density digestive health immune support joint support reproduction fertility skin allergy weight gain weight loss product view cart account info request catalog address book home equine equine total horse health athletic performance bone health digestive health hoof support immune support joint support metabolic support reproduction fertility skin allergy support weight gain weight loss product home contact request catalog toll free am pm pst monday friday copyright platinum performance inc\n",
      " ad-lose ad-weight ad-fast ad-read ad-lost ad-pound ad-fat ad-month ad-www ad-women ad-health ad-com title-healthy title-diet title-plan title-busy title-lifestyle title-food title-diet title-review header-healthy header-diet header-plan header-lifestyle header-women header-health header-fitness healthy diet plan busy lifestyle food diet review home stay date contact site map diet review book review healthy lifestyle fitness healthy diet incredible edible daily life stress management skin care women health article healthy diet plan lifestyle women health fitness look feel live healthy diet plan confuse jargon healthy diet fitness wellness vast range information try carb diet low fat diet difference zone diet atkin diet south beach dier complicate doesn research save time answer healthy diet plan benefit accurate easy understand information importance achieve healthy diet plan lifestyle women health risk reason start mak change poor choice nutrition fitness lifestyle affect women health heart disease strok diabete osteoporosis cancer disease affect women health world wide medical science continue indisputable benefit regular exercise manage healthy diet prevention disease women fitness health increase priority women world stay inform proactive lifestyle dramatically change health health change overall perspective life achieve healthy lifestyle feel look live mak feel person tak action benefit feel short time late improve health lifestyle answer healthy diet plan lifestyle choose healthy diet live healthy lifestyle fitness life daily life discover diet review book review stress management nutrition healthy skin care health article ll article answer question various health issue women health goal help achieve healthy lifestyle health severely limit re able life research stay inform achieve healthy lifestyle women health fitness review diet review book review lifestyle healthy lifestyle fitness life stress management daily life diet nutrition healthy diet choice nutrition weight management incredible edible skin care skin care skindisorder women health women health article subscribe twh article diabete heart healthy meal provide plus delicious recipe people ve look portion healthier recipe book power adversity written al weatherhead fre feldman philanthropist business leader mindset help succeed blueprint help accomplish own success popular page woman heart pertinent information step help women recognize heart disease mak choice prevent heart disease diagnosis treatment nutrisystem reiki meditation balance body pilate beginner start blueberry delicious powerful nutritious author pick review top nutrition book diabete inside track challenge diabete overpower share page enjoy page please pay forward prefer share page link click html link code below copy paste ad note own blog web page forums blog comment facebook account page valuable return top home stay date book review healthy lifestyle daily life stress management nutrition healthy diet incredible edible fitness weight management diet review skin care skin disorder women health sitesearch site map contact resource site policy copyright reserve women health com website design\n",
      " ad-free ad-stock ad-photo ad-create ad-free ad-account ad-download ad-resolution ad-image ad-free title-stock title-photography title-download title-free title-stock title-photo title-royalty title-free title-image header-dreamstime stock photography download free stock photo royalty free image phone email dreamstime english english deutsch espa ol fran aise italiano nederland portugu sign dreamstime free buy image username password remember forgot password search image advance search dreamstime download quality royalty free image low image free designer sign free access lowest price royalty free stock photography buy credit download stock photo resolution stock image reach low free immediately download huge collection royalty free image thousand stock photo ad daily photographer sell stock image sale join powerful photo community reach stock photography market upload image faq contact term api referral program site map privacy policy support phone click contact copyright dreamstime reserve dreamstime paca cepic\n",
      "\n",
      "Some non-relevant ads:\n",
      " ad-top ad-crohn ad-sign ad-crohn ad-disease ad-doctor ad-tel title-symptom title-crohn title-disease header-symptom header-crohn header-disease symptom crohn disease home symptom test diagnosis complication cause risk factor cop support contact symptom\n",
      " ad-dairy ad-milk ad-production ad-elanco ad-safe ad-healthy ad-product ad-improve ad-dairy ad-cow ad-milk ad-production ad-elancodairy ad-com title-elanco title-dairy title-product header-elanco header-dairy header-product elanco dairy product milk hun\n",
      " ad-eye ad-dark ad-circle ad-eye ad-treatment ad-review title-eye title-treatment title-review title-effective title-solution title-dark title-circle title-puffy title-ey title-fine title-line title-wrinkle eye treatment review effective solution dark cr fair fair teamine fair fair fair fair bigatti restoration eye return fair fair fair fair eye treatment clinically test proven independent clinical test extremely expensive perform product result product clinically test expensive competitor independent clinical test verifiable determine product truly effective inevitably happen intend eye product spend budget create package advertise product unfortunately leave money budget product test unfortunate modern day consumer able past design label determine ingredient inside worth money home review eye treatment term privacy payment contact\n",
      " ad-sperm ad-bank ad-excellent ad-sperm ad-donor ad-feature ad-donor ad-free ad-information title-european title-sperm title-bank title-usa title-donor title-list header-european header-sperm header-bank header-usa header-donor header-list european sperue blond civil engineer graduate student available late october rh ici iui id dwight caucasian swedish irish scottish danish blue light brown phd molecular biology post doctorate research fellow available september rh ici iui id edmund caucasion english german green brown brown engineer business cpa rh ici iui id edward caucasian german french danish blue green brown mechanical engineer rh ici iui id elijah caucasian german brown brown ma economics graduate student rh ici iui id fabian caucasian english swiss blue blond bfa illustration design art director rh ici iui id gordon caucasian norwegian swedish irish scottish green brown dark brown engineer technology executor sr vice president rh ici iui id harvey caucasian welsh english french danish blue brown criminal justice national account representative available october rh ici iui id javier mix italian uruguayan brazilian german brown brown biochemistry molecular biology md mba joint program student rh ici iui id jeremy caucasian italian polish german blue blond zoology philosophy student ab rh ici iui id jessie asian mix filipino german brown black graphic design rh ici iui id jordan caucasian polish french welsh green light brown ph science post doctorate reseach fellow rh ici iui id joseph caucasian french english green brown dark brown market communication sale available september rh ici iui id joshua caucasian italian english scottish irish blue green light brown business major rh ici iui id julian caucasian german polish blue green light brown theatre art youth program available september rh ici iui id junjie asian chinese brown black engineer database engineer available september rh ici iui id justin caucasian russian german mix green brown light brown ph biology graduate student rh ici iui id lennie african american welsh scottish brown black biology available late august rh ici iui id manuel mix dominican irish italian brown dark brown computer science rh ici iui id marcus caucasian german swedish dutch blue blond biology rh ici iui id martin caucasianscottish swedish blue green dark brown sociology psychology minor rh ici iui id mathew mix english german japanese green brown darkbrown environmental science rh ici iui id matia caucasian german english blue brown graphic design fine art graphic designer rh ici iui id milton caucasian irish norwegian blue blond law rh ici iui id morgan asian korean brown black medical doctor rh ici iui id nathan caucasian german blue blond ph engineer graduate student rh ici iui id olivercaucasian dutch blue blond finance available sale outside rh ici iui id robbie caucasian northern european alaskan native brown dark brown civil environmental engineer rh ici iui id ronald caucasian english blue brown criminal justice social social supervisor rh ici iui id samson caucasian dutch italiangreenbrown light brown biology pre med music rh ici iui id sidney mix taiwanese german green dark brown fine art science rh ici iui id stefan caucasian italian german irish brown dark brown associate art available september rh ici iui id steven caucasian irish scottish blue brown basic mechanical science pharmacy graduate student rh ici iui id taylor asian korean chinese brown black biochemistry rh ici iui id tobia caucasian german french green brown political science history executor profit organization available beginine october rh ici iui id trevor caucasian english french scottish irish blue light brown graphic design rh ici iui id victor caucasian hawaiian english mix blue brown english major rh iciiui id wentao asianchinese brown black human food nutrition physical wellness consultant ab ici iui id wesley caucasian irish english scottish blue light brown science insurance adjuster rh iciiui id xander caucasian welsh scottish green blue light brown finance financial analyst rh ici iui id european sperm bank usa th avenue ne suite seattle wa ph info europeanspermbankusa com european sperm bank usa donor list\n",
      " ad-fibromyalgia ad-disease ad-deal ad-fibromyalgia ad-don title-fibromyalgia title-dalla title-massage title-pain title-relief title-flexibility header-john header-jame header-massage header-plano header-texa header-fibromyalgia header-dalla header-maslitate fibromyalgia support happy discuss personal circumstance fibromyalgia symptom please call able help obligation john jame lmt john jame massage plano share del icio stumble share stumbleupon share technorati share facebook tweet subscribe comment post relate post myofascial release massage pain craniosacral therapy energy balance post fil craniosacral therapy fibromyalgia massage myofascial massage myofascial release comment response fibromyalgia dalla massage pain relief flexibility chicago school thai massage massage beauty wisdom february th pm fibromyalgia dalla massage pain relief flexibility spoelstra massage therapy august th am agree myofascial massage help lot release fatigue stress body specially relieve body pain healthy body healthy mind post cheryl tucker august th am information helpful learn lot read job john name require email address require website speak mind schedule massage call relate topics sport massage therapy benefit swimmer pre post event reflexology address pain cancer patient ease fibromyalgia dalla massage pain relief flexibility massage therapy pharoah plano texa myofascial release massage pain myofascial massage release pain myofascial release specialize massage therapy lengthen body muscle connective tissue relieve pain common soft tissue disorder leave myofascial release massage comfortable relax breathing deeply read fibromyalgia dalla massage pain relief flexibility fibromyalgia massage therapy popular fibromyalgia patient survey people fibromyalgia include dalla fibromyalgia respondent significant majority consider massage therapy treatment option choice read craniosacral therapy energy balance craniosacral therapy cst focuse improve performance central nervous system fre circulation cerebrospinal fluid produce brain ventricle fluid cushion brain spinal cord pulse throughout body light touch craniosacral therapist relax restriction read juice plus body nutrition meet demand click learn recent post sport massage therapy benefit swimmer pre post event reflexology address pain cancer patient ease fibromyalgia dalla massage pain relief flexibility massage therapy pharoah plano texa myofascial release massage pain craniosacral therapy energy balance recommend link american massage therapy association dentist sherman texa frisco chiropractor john barn myofascial release authority mesothelioma cancer upledger institute craniosacral therapy authority twitter realtime copyright john jame massage plano texa reserve seo website development pontarae logswitch mobile site\n"
     ]
    }
   ],
   "source": [
    "# Sample some relevant and non-relevant ads\n",
    "relevant_ads = df[df['Label'] == 1].sample(5)['Text'].tolist()\n",
    "non_relevant_ads = df[df['Label'] == -1].sample(5)['Text'].tolist()\n",
    "\n",
    "print(\"\\nSome relevant ads:\")\n",
    "for ad in relevant_ads:\n",
    "    print(ad)\n",
    "\n",
    "print(\"\\nSome non-relevant ads:\")\n",
    "for ad in non_relevant_ads:\n",
    "    print(ad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndocunments = df[\\'Text\\'].tolist()\\n\\n# Text preprocessing function\\ndef preprocess_text(text):\\n    # Tokenization\\n    tokens = word_tokenize(text)\\n    \\n    # Removing punctuation and lowercasing\\n    tokens = [word.lower() for word in tokens if word.isalpha()]\\n    \\n    # Removing stop words\\n    stop_words = set(ENGLISH_STOP_WORDS)\\n    tokens = [word for word in tokens if word not in stop_words]\\n    \\n    # Lemmatization\\n    lemmatizer = WordNetLemmatizer()\\n    tokens = [lemmatizer.lemmatize(word) for word in tokens]\\n    \\n    return \\' \\'.join(tokens)\\n\\n# Preprocess the documents\\npreprocessed_documents = [preprocess_text(doc) for doc in docunments]\\n\\n# Create a CountVectorizer to obtain the term-document matrix\\nvectorizer = CountVectorizer()\\nterm_document_matrix = vectorizer.fit_transform(preprocessed_documents)\\n\\n# Create an LDA model\\nnum_topics = 20\\nlda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\\n\\n# Fit the LDA model on the term-document matrix\\nlda_model.fit(term_document_matrix)\\n\\n# Get the topic-document distribution\\ntopic_document_matrix = lda_model.transform(term_document_matrix)\\n\\nprint(\"Term-Document Matrix:\")\\nprint(term_document_matrix.toarray())\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "docunments = df['Text'].tolist()\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Removing punctuation and lowercasing\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Removing stop words\n",
    "    stop_words = set(ENGLISH_STOP_WORDS)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Preprocess the documents\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in docunments]\n",
    "\n",
    "# Create a CountVectorizer to obtain the term-document matrix\n",
    "vectorizer = CountVectorizer()\n",
    "term_document_matrix = vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "# Create an LDA model\n",
    "num_topics = 20\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "\n",
    "# Fit the LDA model on the term-document matrix\n",
    "lda_model.fit(term_document_matrix)\n",
    "\n",
    "# Get the topic-document distribution\n",
    "topic_document_matrix = lda_model.transform(term_document_matrix)\n",
    "\n",
    "print(\"Term-Document Matrix:\")\n",
    "print(term_document_matrix.toarray())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaa</th>\n",
       "      <th>aaaaaaaaaew</th>\n",
       "      <th>aaaaaaaaato</th>\n",
       "      <th>aaaaaaaagy</th>\n",
       "      <th>aaahc</th>\n",
       "      <th>aaai</th>\n",
       "      <th>aaalac</th>\n",
       "      <th>...</th>\n",
       "      <th>zx</th>\n",
       "      <th>zxiuyxnweb</th>\n",
       "      <th>zxwrjqag</th>\n",
       "      <th>zxzg</th>\n",
       "      <th>zy</th>\n",
       "      <th>zydeco</th>\n",
       "      <th>zyla</th>\n",
       "      <th>zymosine</th>\n",
       "      <th>zyrtec</th>\n",
       "      <th>zzay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ad 1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.295645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad 2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232515</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad 3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad 4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad 5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad 4139</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad 4140</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad 4141</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad 4142</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad 4143</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4143 rows × 47513 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          aa       aaa  aaaa  aaaaa  aaaaaaaaaew  aaaaaaaaato  aaaaaaaagy   \n",
       "Ad 1     0.0  0.295645   0.0    0.0          0.0          0.0         0.0  \\\n",
       "Ad 2     0.0  0.232515   0.0    0.0          0.0          0.0         0.0   \n",
       "Ad 3     0.0  0.000000   0.0    0.0          0.0          0.0         0.0   \n",
       "Ad 4     0.0  0.000000   0.0    0.0          0.0          0.0         0.0   \n",
       "Ad 5     0.0  0.000000   0.0    0.0          0.0          0.0         0.0   \n",
       "...      ...       ...   ...    ...          ...          ...         ...   \n",
       "Ad 4139  0.0  0.000000   0.0    0.0          0.0          0.0         0.0   \n",
       "Ad 4140  0.0  0.000000   0.0    0.0          0.0          0.0         0.0   \n",
       "Ad 4141  0.0  0.000000   0.0    0.0          0.0          0.0         0.0   \n",
       "Ad 4142  0.0  0.000000   0.0    0.0          0.0          0.0         0.0   \n",
       "Ad 4143  0.0  0.000000   0.0    0.0          0.0          0.0         0.0   \n",
       "\n",
       "         aaahc  aaai  aaalac  ...   zx  zxiuyxnweb  zxwrjqag  zxzg   zy   \n",
       "Ad 1       0.0   0.0     0.0  ...  0.0         0.0       0.0   0.0  0.0  \\\n",
       "Ad 2       0.0   0.0     0.0  ...  0.0         0.0       0.0   0.0  0.0   \n",
       "Ad 3       0.0   0.0     0.0  ...  0.0         0.0       0.0   0.0  0.0   \n",
       "Ad 4       0.0   0.0     0.0  ...  0.0         0.0       0.0   0.0  0.0   \n",
       "Ad 5       0.0   0.0     0.0  ...  0.0         0.0       0.0   0.0  0.0   \n",
       "...        ...   ...     ...  ...  ...         ...       ...   ...  ...   \n",
       "Ad 4139    0.0   0.0     0.0  ...  0.0         0.0       0.0   0.0  0.0   \n",
       "Ad 4140    0.0   0.0     0.0  ...  0.0         0.0       0.0   0.0  0.0   \n",
       "Ad 4141    0.0   0.0     0.0  ...  0.0         0.0       0.0   0.0  0.0   \n",
       "Ad 4142    0.0   0.0     0.0  ...  0.0         0.0       0.0   0.0  0.0   \n",
       "Ad 4143    0.0   0.0     0.0  ...  0.0         0.0       0.0   0.0  0.0   \n",
       "\n",
       "         zydeco  zyla  zymosine  zyrtec  zzay  \n",
       "Ad 1        0.0   0.0       0.0     0.0   0.0  \n",
       "Ad 2        0.0   0.0       0.0     0.0   0.0  \n",
       "Ad 3        0.0   0.0       0.0     0.0   0.0  \n",
       "Ad 4        0.0   0.0       0.0     0.0   0.0  \n",
       "Ad 5        0.0   0.0       0.0     0.0   0.0  \n",
       "...         ...   ...       ...     ...   ...  \n",
       "Ad 4139     0.0   0.0       0.0     0.0   0.0  \n",
       "Ad 4140     0.0   0.0       0.0     0.0   0.0  \n",
       "Ad 4141     0.0   0.0       0.0     0.0   0.0  \n",
       "Ad 4142     0.0   0.0       0.0     0.0   0.0  \n",
       "Ad 4143     0.0   0.0       0.0     0.0   0.0  \n",
       "\n",
       "[4143 rows x 47513 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create TF-IDF Vectorizer object\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the ad texts and transform the ad texts into a TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Text'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a pandas DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out(), index=[f\"Ad {i+1}\" for i in range(len(df['Text']))])\n",
    "\n",
    "# Print a sample of the TF-IDF matrix\n",
    "display(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display top terms for each ad in a DataFrame\n",
    "def display_top_terms_df_with_labels(tfidf_matrix, vectorizer, labels, n=5):\n",
    "    # Get feature names (terms)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Convert the TF-IDF matrix to dense array\n",
    "    tfidf_matrix_dense = tfidf_matrix.toarray()\n",
    "    \n",
    "    # Initialize an empty DataFrame to store top terms\n",
    "    top_terms_df = pd.DataFrame(columns=[f\"Term {i+1}\" for i in range(n)] + [\"Label\"], index=[f\"Ad {i+1}\" for i in range(tfidf_matrix_dense.shape[0])])\n",
    "    \n",
    "    # Iterate over each ad\n",
    "    for i in range(tfidf_matrix_dense.shape[0]):\n",
    "        # Get TF-IDF scores for the current ad\n",
    "        ad_tfidf_scores = tfidf_matrix_dense[i]\n",
    "        \n",
    "        # Get indices of top N TF-IDF scores\n",
    "        top_indices = (-ad_tfidf_scores).argsort()[:n]  # Use negative scores for descending order\n",
    "        \n",
    "        # Get top terms and their TF-IDF scores\n",
    "        top_terms = [(feature_names[idx], ad_tfidf_scores[idx]) for idx in top_indices]\n",
    "        \n",
    "        # Store top terms in the DataFrame\n",
    "        for j, (term, score) in enumerate(top_terms):\n",
    "            top_terms_df.at[f\"Ad {i+1}\", f\"Term {j+1}\"] = f\"{term} ({score:.4f})\"\n",
    "        \n",
    "        # Add label to the DataFrame\n",
    "        top_terms_df.at[f\"Ad {i+1}\", \"Label\"] = labels[i]\n",
    "    \n",
    "    return top_terms_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term 1</th>\n",
       "      <th>Term 2</th>\n",
       "      <th>Term 3</th>\n",
       "      <th>Term 4</th>\n",
       "      <th>Term 5</th>\n",
       "      <th>Term 6</th>\n",
       "      <th>Term 7</th>\n",
       "      <th>Term 8</th>\n",
       "      <th>Term 9</th>\n",
       "      <th>Term 10</th>\n",
       "      <th>...</th>\n",
       "      <th>Term 12</th>\n",
       "      <th>Term 13</th>\n",
       "      <th>Term 14</th>\n",
       "      <th>Term 15</th>\n",
       "      <th>Term 16</th>\n",
       "      <th>Term 17</th>\n",
       "      <th>Term 18</th>\n",
       "      <th>Term 19</th>\n",
       "      <th>Term 20</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Ad 1</th>\n",
       "      <td>ad (0.5301)</td>\n",
       "      <td>findtheaaanswer (0.3522)</td>\n",
       "      <td>doctorfinder (0.3356)</td>\n",
       "      <td>aneurysm (0.3239)</td>\n",
       "      <td>aortic (0.3239)</td>\n",
       "      <td>aaa (0.2956)</td>\n",
       "      <td>abdominal (0.1993)</td>\n",
       "      <td>org (0.1778)</td>\n",
       "      <td>physician (0.1720)</td>\n",
       "      <td>patient (0.1391)</td>\n",
       "      <td>...</td>\n",
       "      <td>treat (0.1291)</td>\n",
       "      <td>found (0.1024)</td>\n",
       "      <td>www (0.0847)</td>\n",
       "      <td>page (0.0844)</td>\n",
       "      <td>help (0.0796)</td>\n",
       "      <td>perstorp (0.0000)</td>\n",
       "      <td>pertain (0.0000)</td>\n",
       "      <td>perth (0.0000)</td>\n",
       "      <td>pertinent (0.0000)</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad 2</th>\n",
       "      <td>aaanswer (0.5279)</td>\n",
       "      <td>installation (0.2941)</td>\n",
       "      <td>application (0.2887)</td>\n",
       "      <td>flash (0.2783)</td>\n",
       "      <td>player (0.2705)</td>\n",
       "      <td>ad (0.2566)</td>\n",
       "      <td>aortic (0.2547)</td>\n",
       "      <td>aneurysm (0.2547)</td>\n",
       "      <td>undiagnose (0.2417)</td>\n",
       "      <td>aaa (0.2325)</td>\n",
       "      <td>...</td>\n",
       "      <td>abdominal (0.1567)</td>\n",
       "      <td>million (0.1268)</td>\n",
       "      <td>american (0.1091)</td>\n",
       "      <td>live (0.0815)</td>\n",
       "      <td>perth (0.0000)</td>\n",
       "      <td>pertain (0.0000)</td>\n",
       "      <td>pertinent (0.0000)</td>\n",
       "      <td>pertussis (0.0000)</td>\n",
       "      <td>pertussisawareness (0.0000)</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad 3</th>\n",
       "      <td>spill (0.5289)</td>\n",
       "      <td>dolor (0.2841)</td>\n",
       "      <td>refill (0.2242)</td>\n",
       "      <td>aliquet (0.1987)</td>\n",
       "      <td>nunc (0.1987)</td>\n",
       "      <td>kit (0.1777)</td>\n",
       "      <td>que (0.1641)</td>\n",
       "      <td>source (0.1561)</td>\n",
       "      <td>manufacturer (0.1045)</td>\n",
       "      <td>ad (0.1036)</td>\n",
       "      <td>...</td>\n",
       "      <td>maure (0.0994)</td>\n",
       "      <td>nibh (0.0994)</td>\n",
       "      <td>adipisc (0.0994)</td>\n",
       "      <td>nullam (0.0994)</td>\n",
       "      <td>egesta (0.0994)</td>\n",
       "      <td>ipsum (0.0994)</td>\n",
       "      <td>sapien (0.0994)</td>\n",
       "      <td>massa (0.0994)</td>\n",
       "      <td>snar (0.0994)</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad 4</th>\n",
       "      <td>reflux (0.5024)</td>\n",
       "      <td>title (0.3935)</td>\n",
       "      <td>acidreflux (0.3153)</td>\n",
       "      <td>acid (0.3070)</td>\n",
       "      <td>ad (0.3065)</td>\n",
       "      <td>headline (0.2427)</td>\n",
       "      <td>heartburn (0.2181)</td>\n",
       "      <td>treatment (0.1790)</td>\n",
       "      <td>header (0.1724)</td>\n",
       "      <td>relief (0.1481)</td>\n",
       "      <td>...</td>\n",
       "      <td>solution (0.1251)</td>\n",
       "      <td>option (0.1194)</td>\n",
       "      <td>cause (0.1142)</td>\n",
       "      <td>article (0.1137)</td>\n",
       "      <td>symptom (0.1110)</td>\n",
       "      <td>disease (0.1033)</td>\n",
       "      <td>top (0.0980)</td>\n",
       "      <td>information (0.0791)</td>\n",
       "      <td>com (0.0612)</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad 5</th>\n",
       "      <td>reflux (0.6101)</td>\n",
       "      <td>acid (0.3728)</td>\n",
       "      <td>ad (0.3101)</td>\n",
       "      <td>search (0.2813)</td>\n",
       "      <td>header (0.2791)</td>\n",
       "      <td>title (0.2124)</td>\n",
       "      <td>healthdesk (0.2123)</td>\n",
       "      <td>symptom (0.1797)</td>\n",
       "      <td>php (0.1782)</td>\n",
       "      <td>result (0.1609)</td>\n",
       "      <td>...</td>\n",
       "      <td>topics (0.1249)</td>\n",
       "      <td>expert (0.0917)</td>\n",
       "      <td>health (0.0705)</td>\n",
       "      <td>com (0.0496)</td>\n",
       "      <td>persprofessional (0.0000)</td>\n",
       "      <td>pertain (0.0000)</td>\n",
       "      <td>perspiration (0.0000)</td>\n",
       "      <td>perth (0.0000)</td>\n",
       "      <td>pertinent (0.0000)</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad 4139</th>\n",
       "      <td>aquarium (0.7467)</td>\n",
       "      <td>dog (0.2826)</td>\n",
       "      <td>reptile (0.2772)</td>\n",
       "      <td>pond (0.1977)</td>\n",
       "      <td>food (0.1972)</td>\n",
       "      <td>filter (0.1864)</td>\n",
       "      <td>cat (0.1755)</td>\n",
       "      <td>bird (0.1282)</td>\n",
       "      <td>bulb (0.1229)</td>\n",
       "      <td>pet (0.0946)</td>\n",
       "      <td>...</td>\n",
       "      <td>terrarium (0.0700)</td>\n",
       "      <td>treat (0.0669)</td>\n",
       "      <td>pump (0.0582)</td>\n",
       "      <td>toy (0.0504)</td>\n",
       "      <td>cleaner (0.0501)</td>\n",
       "      <td>water (0.0493)</td>\n",
       "      <td>media (0.0466)</td>\n",
       "      <td>litter (0.0458)</td>\n",
       "      <td>decoration (0.0440)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad 4140</th>\n",
       "      <td>aquarium (0.7473)</td>\n",
       "      <td>dog (0.2792)</td>\n",
       "      <td>reptile (0.2731)</td>\n",
       "      <td>food (0.2011)</td>\n",
       "      <td>pond (0.1967)</td>\n",
       "      <td>cat (0.1832)</td>\n",
       "      <td>filter (0.1824)</td>\n",
       "      <td>bird (0.1298)</td>\n",
       "      <td>bulb (0.1223)</td>\n",
       "      <td>pet (0.0941)</td>\n",
       "      <td>...</td>\n",
       "      <td>terrarium (0.0696)</td>\n",
       "      <td>treat (0.0687)</td>\n",
       "      <td>pump (0.0579)</td>\n",
       "      <td>toy (0.0502)</td>\n",
       "      <td>cleaner (0.0499)</td>\n",
       "      <td>water (0.0491)</td>\n",
       "      <td>litter (0.0488)</td>\n",
       "      <td>media (0.0463)</td>\n",
       "      <td>decoration (0.0437)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad 4141</th>\n",
       "      <td>aquarium (0.7479)</td>\n",
       "      <td>dog (0.2795)</td>\n",
       "      <td>reptile (0.2733)</td>\n",
       "      <td>food (0.2012)</td>\n",
       "      <td>pond (0.1969)</td>\n",
       "      <td>cat (0.1834)</td>\n",
       "      <td>filter (0.1825)</td>\n",
       "      <td>bird (0.1254)</td>\n",
       "      <td>bulb (0.1224)</td>\n",
       "      <td>pet (0.0942)</td>\n",
       "      <td>...</td>\n",
       "      <td>terrarium (0.0697)</td>\n",
       "      <td>treat (0.0688)</td>\n",
       "      <td>pump (0.0580)</td>\n",
       "      <td>toy (0.0502)</td>\n",
       "      <td>cleaner (0.0499)</td>\n",
       "      <td>water (0.0491)</td>\n",
       "      <td>litter (0.0488)</td>\n",
       "      <td>media (0.0464)</td>\n",
       "      <td>decoration (0.0438)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad 4142</th>\n",
       "      <td>aquarium (0.7449)</td>\n",
       "      <td>dog (0.2783)</td>\n",
       "      <td>reptile (0.2722)</td>\n",
       "      <td>food (0.2021)</td>\n",
       "      <td>pond (0.1961)</td>\n",
       "      <td>cat (0.1826)</td>\n",
       "      <td>filter (0.1818)</td>\n",
       "      <td>bird (0.1248)</td>\n",
       "      <td>bulb (0.1219)</td>\n",
       "      <td>zupreem (0.1110)</td>\n",
       "      <td>...</td>\n",
       "      <td>terrarium (0.0694)</td>\n",
       "      <td>treat (0.0685)</td>\n",
       "      <td>pump (0.0577)</td>\n",
       "      <td>parrot (0.0571)</td>\n",
       "      <td>toy (0.0500)</td>\n",
       "      <td>cleaner (0.0497)</td>\n",
       "      <td>water (0.0489)</td>\n",
       "      <td>litter (0.0486)</td>\n",
       "      <td>media (0.0462)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ad 4143</th>\n",
       "      <td>aquarium (0.7477)</td>\n",
       "      <td>dog (0.2793)</td>\n",
       "      <td>reptile (0.2732)</td>\n",
       "      <td>food (0.2028)</td>\n",
       "      <td>pond (0.1968)</td>\n",
       "      <td>cat (0.1833)</td>\n",
       "      <td>filter (0.1825)</td>\n",
       "      <td>bird (0.1253)</td>\n",
       "      <td>bulb (0.1223)</td>\n",
       "      <td>pet (0.0942)</td>\n",
       "      <td>...</td>\n",
       "      <td>terrarium (0.0697)</td>\n",
       "      <td>treat (0.0687)</td>\n",
       "      <td>pump (0.0580)</td>\n",
       "      <td>toy (0.0502)</td>\n",
       "      <td>cleaner (0.0499)</td>\n",
       "      <td>water (0.0491)</td>\n",
       "      <td>litter (0.0488)</td>\n",
       "      <td>media (0.0464)</td>\n",
       "      <td>decoration (0.0438)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4143 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Term 1                    Term 2                 Term 3   \n",
       "Ad 1           ad (0.5301)  findtheaaanswer (0.3522)  doctorfinder (0.3356)  \\\n",
       "Ad 2     aaanswer (0.5279)     installation (0.2941)   application (0.2887)   \n",
       "Ad 3        spill (0.5289)            dolor (0.2841)        refill (0.2242)   \n",
       "Ad 4       reflux (0.5024)            title (0.3935)    acidreflux (0.3153)   \n",
       "Ad 5       reflux (0.6101)             acid (0.3728)            ad (0.3101)   \n",
       "...                    ...                       ...                    ...   \n",
       "Ad 4139  aquarium (0.7467)              dog (0.2826)       reptile (0.2772)   \n",
       "Ad 4140  aquarium (0.7473)              dog (0.2792)       reptile (0.2731)   \n",
       "Ad 4141  aquarium (0.7479)              dog (0.2795)       reptile (0.2733)   \n",
       "Ad 4142  aquarium (0.7449)              dog (0.2783)       reptile (0.2722)   \n",
       "Ad 4143  aquarium (0.7477)              dog (0.2793)       reptile (0.2732)   \n",
       "\n",
       "                    Term 4           Term 5             Term 6   \n",
       "Ad 1     aneurysm (0.3239)  aortic (0.3239)       aaa (0.2956)  \\\n",
       "Ad 2        flash (0.2783)  player (0.2705)        ad (0.2566)   \n",
       "Ad 3      aliquet (0.1987)    nunc (0.1987)       kit (0.1777)   \n",
       "Ad 4         acid (0.3070)      ad (0.3065)  headline (0.2427)   \n",
       "Ad 5       search (0.2813)  header (0.2791)     title (0.2124)   \n",
       "...                    ...              ...                ...   \n",
       "Ad 4139      pond (0.1977)    food (0.1972)    filter (0.1864)   \n",
       "Ad 4140      food (0.2011)    pond (0.1967)       cat (0.1832)   \n",
       "Ad 4141      food (0.2012)    pond (0.1969)       cat (0.1834)   \n",
       "Ad 4142      food (0.2021)    pond (0.1961)       cat (0.1826)   \n",
       "Ad 4143      food (0.2028)    pond (0.1968)       cat (0.1833)   \n",
       "\n",
       "                      Term 7              Term 8                 Term 9   \n",
       "Ad 1      abdominal (0.1993)        org (0.1778)     physician (0.1720)  \\\n",
       "Ad 2         aortic (0.2547)   aneurysm (0.2547)    undiagnose (0.2417)   \n",
       "Ad 3            que (0.1641)     source (0.1561)  manufacturer (0.1045)   \n",
       "Ad 4      heartburn (0.2181)  treatment (0.1790)        header (0.1724)   \n",
       "Ad 5     healthdesk (0.2123)    symptom (0.1797)           php (0.1782)   \n",
       "...                      ...                 ...                    ...   \n",
       "Ad 4139         cat (0.1755)       bird (0.1282)          bulb (0.1229)   \n",
       "Ad 4140      filter (0.1824)       bird (0.1298)          bulb (0.1223)   \n",
       "Ad 4141      filter (0.1825)       bird (0.1254)          bulb (0.1224)   \n",
       "Ad 4142      filter (0.1818)       bird (0.1248)          bulb (0.1219)   \n",
       "Ad 4143      filter (0.1825)       bird (0.1253)          bulb (0.1223)   \n",
       "\n",
       "                  Term 10  ...             Term 12           Term 13   \n",
       "Ad 1     patient (0.1391)  ...      treat (0.1291)    found (0.1024)  \\\n",
       "Ad 2         aaa (0.2325)  ...  abdominal (0.1567)  million (0.1268)   \n",
       "Ad 3          ad (0.1036)  ...      maure (0.0994)     nibh (0.0994)   \n",
       "Ad 4      relief (0.1481)  ...   solution (0.1251)   option (0.1194)   \n",
       "Ad 5      result (0.1609)  ...     topics (0.1249)   expert (0.0917)   \n",
       "...                   ...  ...                 ...               ...   \n",
       "Ad 4139      pet (0.0946)  ...  terrarium (0.0700)    treat (0.0669)   \n",
       "Ad 4140      pet (0.0941)  ...  terrarium (0.0696)    treat (0.0687)   \n",
       "Ad 4141      pet (0.0942)  ...  terrarium (0.0697)    treat (0.0688)   \n",
       "Ad 4142  zupreem (0.1110)  ...  terrarium (0.0694)    treat (0.0685)   \n",
       "Ad 4143      pet (0.0942)  ...  terrarium (0.0697)    treat (0.0687)   \n",
       "\n",
       "                   Term 14           Term 15                    Term 16   \n",
       "Ad 1          www (0.0847)     page (0.0844)              help (0.0796)  \\\n",
       "Ad 2     american (0.1091)     live (0.0815)             perth (0.0000)   \n",
       "Ad 3      adipisc (0.0994)   nullam (0.0994)            egesta (0.0994)   \n",
       "Ad 4        cause (0.1142)  article (0.1137)           symptom (0.1110)   \n",
       "Ad 5       health (0.0705)      com (0.0496)  persprofessional (0.0000)   \n",
       "...                    ...               ...                        ...   \n",
       "Ad 4139      pump (0.0582)      toy (0.0504)           cleaner (0.0501)   \n",
       "Ad 4140      pump (0.0579)      toy (0.0502)           cleaner (0.0499)   \n",
       "Ad 4141      pump (0.0580)      toy (0.0502)           cleaner (0.0499)   \n",
       "Ad 4142      pump (0.0577)   parrot (0.0571)               toy (0.0500)   \n",
       "Ad 4143      pump (0.0580)      toy (0.0502)           cleaner (0.0499)   \n",
       "\n",
       "                   Term 17                Term 18               Term 19   \n",
       "Ad 1     perstorp (0.0000)       pertain (0.0000)        perth (0.0000)  \\\n",
       "Ad 2      pertain (0.0000)     pertinent (0.0000)    pertussis (0.0000)   \n",
       "Ad 3        ipsum (0.0994)        sapien (0.0994)        massa (0.0994)   \n",
       "Ad 4      disease (0.1033)           top (0.0980)  information (0.0791)   \n",
       "Ad 5      pertain (0.0000)  perspiration (0.0000)        perth (0.0000)   \n",
       "...                    ...                    ...                   ...   \n",
       "Ad 4139     water (0.0493)         media (0.0466)       litter (0.0458)   \n",
       "Ad 4140     water (0.0491)        litter (0.0488)        media (0.0463)   \n",
       "Ad 4141     water (0.0491)        litter (0.0488)        media (0.0464)   \n",
       "Ad 4142   cleaner (0.0497)         water (0.0489)       litter (0.0486)   \n",
       "Ad 4143     water (0.0491)        litter (0.0488)        media (0.0464)   \n",
       "\n",
       "                             Term 20 Label  \n",
       "Ad 1              pertinent (0.0000)    -1  \n",
       "Ad 2     pertussisawareness (0.0000)    -1  \n",
       "Ad 3                   snar (0.0994)    -1  \n",
       "Ad 4                    com (0.0612)    -1  \n",
       "Ad 5              pertinent (0.0000)    -1  \n",
       "...                              ...   ...  \n",
       "Ad 4139          decoration (0.0440)     1  \n",
       "Ad 4140          decoration (0.0437)     1  \n",
       "Ad 4141          decoration (0.0438)     1  \n",
       "Ad 4142               media (0.0462)     1  \n",
       "Ad 4143          decoration (0.0438)     1  \n",
       "\n",
       "[4143 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "top_terms_df_with_labels = display_top_terms_df_with_labels(tfidf_matrix, tfidf_vectorizer, df['Label'], n=20)\n",
    "display(top_terms_df_with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             S1  S2  S3  S4  S5  S6  S7  S8  S9  S10  ...  S4134  S4135   \n",
      "aa            0   0   0   0   0   0   0   0   0    0  ...      0      0  \\\n",
      "aaa           1   1   0   0   0   0   0   0   0    0  ...      0      0   \n",
      "aaaa          0   0   0   0   0   0   0   0   0    0  ...      0      0   \n",
      "aaaaa         0   0   0   0   0   0   0   0   0    0  ...      0      0   \n",
      "aaaaaaaaaew   0   0   0   0   0   0   0   0   0    0  ...      0      0   \n",
      "...          ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ...    ...    ...   \n",
      "zydeco        0   0   0   0   0   0   0   0   0    0  ...      0      0   \n",
      "zyla          0   0   0   0   0   0   0   0   0    0  ...      0      0   \n",
      "zymosine      0   0   0   0   0   0   0   0   0    0  ...      0      0   \n",
      "zyrtec        0   0   0   0   0   0   0   0   0    0  ...      0      0   \n",
      "zzay          0   0   0   0   0   0   0   0   0    0  ...      0      0   \n",
      "\n",
      "             S4136  S4137  S4138  S4139  S4140  S4141  S4142  S4143  \n",
      "aa               0      0      0      0      0      0      0      0  \n",
      "aaa              0      0      0      0      0      0      0      0  \n",
      "aaaa             0      0      0      0      0      0      0      0  \n",
      "aaaaa            0      0      0      0      0      0      0      0  \n",
      "aaaaaaaaaew      0      0      0      0      0      0      0      0  \n",
      "...            ...    ...    ...    ...    ...    ...    ...    ...  \n",
      "zydeco           0      0      0      0      0      0      0      0  \n",
      "zyla             0      0      0      0      0      0      0      0  \n",
      "zymosine         0      0      0      0      0      0      0      0  \n",
      "zyrtec           0      0      0      0      0      0      0      0  \n",
      "zzay             0      0      0      0      0      0      0      0  \n",
      "\n",
      "[47513 rows x 4143 columns]\n"
     ]
    }
   ],
   "source": [
    "# Learn features based on text\n",
    "count_vect = CountVectorizer()\n",
    "counts_1 = count_vect.fit_transform(df['Text'])\n",
    "\n",
    "printTermDocumentMatrix(count_vect, counts_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             S1  S2  S3  S4  S5  S6  S7  S8  S9  S10  ...  S4134  S4135   \n",
      "aa            0   0   0   0   0   0   0   0   0    0  ...      0      0  \\\n",
      "aaa           0   0   0   0   0   0   0   0   0    0  ...      0      0   \n",
      "aaaa          0   0   0   0   0   0   0   0   0    0  ...      0      0   \n",
      "aaaaa         0   0   0   0   0   0   0   0   0    0  ...      0      0   \n",
      "aaaaaaaaaew   0   0   0   0   0   0   0   0   0    0  ...      0      0   \n",
      "...          ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ...    ...    ...   \n",
      "zydeco        0   0   0   0   0   0   0   0   0    0  ...      0      0   \n",
      "zyla          0   0   0   0   0   0   0   0   0    0  ...      0      0   \n",
      "zymosin       0   0   0   0   0   0   0   0   0    0  ...      0      0   \n",
      "zyrtec        0   0   0   0   0   0   0   0   0    0  ...      0      0   \n",
      "zzay          0   0   0   0   0   0   0   0   0    0  ...      0      0   \n",
      "\n",
      "             S4136  S4137  S4138  S4139  S4140  S4141  S4142  S4143  \n",
      "aa               0      0      0      0      0      0      0      0  \n",
      "aaa              0      0      0      0      0      0      0      0  \n",
      "aaaa             0      0      0      0      0      0      0      0  \n",
      "aaaaa            0      0      0      0      0      0      0      0  \n",
      "aaaaaaaaaew      0      0      0      0      0      0      0      0  \n",
      "...            ...    ...    ...    ...    ...    ...    ...    ...  \n",
      "zydeco           0      0      0      0      0      0      0      0  \n",
      "zyla             0      0      0      0      0      0      0      0  \n",
      "zymosin          0      0      0      0      0      0      0      0  \n",
      "zyrtec           0      0      0      0      0      0      0      0  \n",
      "zzay             0      0      0      0      0      0      0      0  \n",
      "\n",
      "[43458 rows x 4143 columns]\n"
     ]
    }
   ],
   "source": [
    "class LemmaTokenizer:\n",
    "    def __init__(self):\n",
    "        self.stemmer = EnglishStemmer()\n",
    "        self.stopWords = set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc) \n",
    "                if t.isalpha() and t not in self.stopWords]\n",
    "\n",
    "# Learn features based on text\n",
    "count_vect = CountVectorizer(tokenizer=LemmaTokenizer())\n",
    "counts_2 = count_vect.fit_transform(df['Text'])\n",
    "\n",
    "printTermDocumentMatrix(count_vect, counts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   S1        S2   S3   S4   S5   S6   S7   S8   S9  S10  ...   \n",
      "aa           0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  \\\n",
      "aaa          7.383265  7.383265  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "aaaa         0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "aaaaa        0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "aaaaaaaaaew  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "...               ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "zydeco       0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "zyla         0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "zymosine     0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "zyrtec       0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "zzay         0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "\n",
      "             S4134  S4135  S4136  S4137  S4138  S4139  S4140  S4141  S4142   \n",
      "aa             0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \\\n",
      "aaa            0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "aaaa           0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "aaaaa          0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "aaaaaaaaaew    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "...            ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "zydeco         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "zyla           0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "zymosine       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "zyrtec         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "zzay           0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "             S4143  \n",
      "aa             0.0  \n",
      "aaa            0.0  \n",
      "aaaa           0.0  \n",
      "aaaaa          0.0  \n",
      "aaaaaaaaaew    0.0  \n",
      "...            ...  \n",
      "zydeco         0.0  \n",
      "zyla           0.0  \n",
      "zymosine       0.0  \n",
      "zyrtec         0.0  \n",
      "zzay           0.0  \n",
      "\n",
      "[47513 rows x 4143 columns]\n"
     ]
    }
   ],
   "source": [
    "# Apply CountVectorizer and TfidfTransformer sequentially\n",
    "count_vect = CountVectorizer()\n",
    "tfidfTransformer = TfidfTransformer(smooth_idf=False, norm=None)\n",
    "counts_3 = count_vect.fit_transform(df['Text'])\n",
    "tfidf = tfidfTransformer.fit_transform(counts_3)\n",
    "\n",
    "printTermDocumentMatrix(count_vect, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   S1        S2   S3   S4   S5   S6   S7   S8   S9  S10  ...   \n",
      "aa           0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  \\\n",
      "aaa          0.295645  0.232515  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "aaaa         0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "aaaaa        0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "aaaaaaaaaew  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "...               ...       ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "zydeco       0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "zyla         0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "zymosine     0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "zyrtec       0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "zzay         0.000000  0.000000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
      "\n",
      "             S4134  S4135  S4136  S4137  S4138  S4139  S4140  S4141  S4142   \n",
      "aa             0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \\\n",
      "aaa            0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "aaaa           0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "aaaaa          0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "aaaaaaaaaew    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "...            ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
      "zydeco         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "zyla           0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "zymosine       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "zyrtec         0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "zzay           0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
      "\n",
      "             S4143  \n",
      "aa             0.0  \n",
      "aaa            0.0  \n",
      "aaaa           0.0  \n",
      "aaaaa          0.0  \n",
      "aaaaaaaaaew    0.0  \n",
      "...            ...  \n",
      "zydeco         0.0  \n",
      "zyla           0.0  \n",
      "zymosine       0.0  \n",
      "zyrtec         0.0  \n",
      "zzay           0.0  \n",
      "\n",
      "[47513 rows x 4143 columns]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_2 = tfidf_vectorizer.fit_transform(df['Text'])\n",
    "\n",
    "printTermDocumentMatrix(tfidf_vectorizer, tfidf_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = EnglishStemmer()\n",
    "        self.stopWords = set(ENGLISH_STOP_WORDS)\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc) \n",
    "                if t.isalpha() and t not in self.stopWords]\n",
    "\n",
    "preprocessor = CountVectorizer(tokenizer=LemmaTokenizer(), encoding='latin1')\n",
    "preprocessedText = preprocessor.fit_transform(df['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfTransformer = TfidfTransformer()\n",
    "tfidf = tfidfTransformer.fit_transform(preprocessedText)\n",
    "\n",
    "svd = TruncatedSVD(20)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "lsa_tfidf = lsa.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept-Document Matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Concept_1</th>\n",
       "      <th>Concept_2</th>\n",
       "      <th>Concept_3</th>\n",
       "      <th>Concept_4</th>\n",
       "      <th>Concept_5</th>\n",
       "      <th>Concept_6</th>\n",
       "      <th>Concept_7</th>\n",
       "      <th>Concept_8</th>\n",
       "      <th>Concept_9</th>\n",
       "      <th>Concept_10</th>\n",
       "      <th>Concept_11</th>\n",
       "      <th>Concept_12</th>\n",
       "      <th>Concept_13</th>\n",
       "      <th>Concept_14</th>\n",
       "      <th>Concept_15</th>\n",
       "      <th>Concept_16</th>\n",
       "      <th>Concept_17</th>\n",
       "      <th>Concept_18</th>\n",
       "      <th>Concept_19</th>\n",
       "      <th>Concept_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999849</td>\n",
       "      <td>-0.013726</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.009711</td>\n",
       "      <td>0.001872</td>\n",
       "      <td>-0.001286</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>-0.002294</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>-0.000447</td>\n",
       "      <td>-0.000826</td>\n",
       "      <td>-0.001424</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.000569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007587</td>\n",
       "      <td>0.186153</td>\n",
       "      <td>0.008054</td>\n",
       "      <td>-0.299281</td>\n",
       "      <td>-0.056562</td>\n",
       "      <td>0.120939</td>\n",
       "      <td>-0.023285</td>\n",
       "      <td>0.015316</td>\n",
       "      <td>-0.083345</td>\n",
       "      <td>0.090258</td>\n",
       "      <td>0.287872</td>\n",
       "      <td>-0.010525</td>\n",
       "      <td>-0.281764</td>\n",
       "      <td>-0.239309</td>\n",
       "      <td>0.315574</td>\n",
       "      <td>0.059115</td>\n",
       "      <td>-0.006630</td>\n",
       "      <td>0.140469</td>\n",
       "      <td>-0.657017</td>\n",
       "      <td>-0.260368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.016368</td>\n",
       "      <td>0.493583</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>-0.720119</td>\n",
       "      <td>-0.187431</td>\n",
       "      <td>0.271094</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>0.086290</td>\n",
       "      <td>0.074131</td>\n",
       "      <td>-0.047149</td>\n",
       "      <td>-0.164009</td>\n",
       "      <td>-0.112872</td>\n",
       "      <td>0.063445</td>\n",
       "      <td>-0.042544</td>\n",
       "      <td>0.161560</td>\n",
       "      <td>-0.104036</td>\n",
       "      <td>-0.000914</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>-0.175392</td>\n",
       "      <td>-0.024019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>0.006211</td>\n",
       "      <td>0.382960</td>\n",
       "      <td>-0.022118</td>\n",
       "      <td>-0.184783</td>\n",
       "      <td>-0.006990</td>\n",
       "      <td>0.109327</td>\n",
       "      <td>0.121565</td>\n",
       "      <td>-0.055678</td>\n",
       "      <td>-0.071760</td>\n",
       "      <td>-0.018371</td>\n",
       "      <td>-0.058241</td>\n",
       "      <td>0.016078</td>\n",
       "      <td>-0.220787</td>\n",
       "      <td>0.841823</td>\n",
       "      <td>0.129586</td>\n",
       "      <td>-0.017570</td>\n",
       "      <td>0.020281</td>\n",
       "      <td>0.015416</td>\n",
       "      <td>0.035358</td>\n",
       "      <td>0.056915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4139</th>\n",
       "      <td>0.006191</td>\n",
       "      <td>0.382418</td>\n",
       "      <td>-0.022140</td>\n",
       "      <td>-0.183659</td>\n",
       "      <td>-0.005000</td>\n",
       "      <td>0.110682</td>\n",
       "      <td>0.120910</td>\n",
       "      <td>-0.055650</td>\n",
       "      <td>-0.074001</td>\n",
       "      <td>-0.018629</td>\n",
       "      <td>-0.058473</td>\n",
       "      <td>0.017759</td>\n",
       "      <td>-0.218048</td>\n",
       "      <td>0.842704</td>\n",
       "      <td>0.129941</td>\n",
       "      <td>-0.016752</td>\n",
       "      <td>0.020332</td>\n",
       "      <td>0.014877</td>\n",
       "      <td>0.035035</td>\n",
       "      <td>0.056799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4140</th>\n",
       "      <td>0.006184</td>\n",
       "      <td>0.381924</td>\n",
       "      <td>-0.022072</td>\n",
       "      <td>-0.183385</td>\n",
       "      <td>-0.004627</td>\n",
       "      <td>0.109066</td>\n",
       "      <td>0.121102</td>\n",
       "      <td>-0.055075</td>\n",
       "      <td>-0.073806</td>\n",
       "      <td>-0.018323</td>\n",
       "      <td>-0.057845</td>\n",
       "      <td>0.018447</td>\n",
       "      <td>-0.218720</td>\n",
       "      <td>0.843001</td>\n",
       "      <td>0.130303</td>\n",
       "      <td>-0.016602</td>\n",
       "      <td>0.020983</td>\n",
       "      <td>0.015862</td>\n",
       "      <td>0.034881</td>\n",
       "      <td>0.056927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4141</th>\n",
       "      <td>0.006197</td>\n",
       "      <td>0.384153</td>\n",
       "      <td>-0.022106</td>\n",
       "      <td>-0.186593</td>\n",
       "      <td>-0.004606</td>\n",
       "      <td>0.111697</td>\n",
       "      <td>0.122782</td>\n",
       "      <td>-0.054554</td>\n",
       "      <td>-0.079079</td>\n",
       "      <td>-0.019551</td>\n",
       "      <td>-0.059865</td>\n",
       "      <td>0.016153</td>\n",
       "      <td>-0.216957</td>\n",
       "      <td>0.841026</td>\n",
       "      <td>0.125826</td>\n",
       "      <td>-0.017966</td>\n",
       "      <td>0.019164</td>\n",
       "      <td>0.014654</td>\n",
       "      <td>0.039665</td>\n",
       "      <td>0.057861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4142</th>\n",
       "      <td>0.006191</td>\n",
       "      <td>0.381609</td>\n",
       "      <td>-0.022023</td>\n",
       "      <td>-0.183640</td>\n",
       "      <td>-0.004929</td>\n",
       "      <td>0.108320</td>\n",
       "      <td>0.120766</td>\n",
       "      <td>-0.053291</td>\n",
       "      <td>-0.074801</td>\n",
       "      <td>-0.018298</td>\n",
       "      <td>-0.057878</td>\n",
       "      <td>0.018845</td>\n",
       "      <td>-0.218416</td>\n",
       "      <td>0.843360</td>\n",
       "      <td>0.130181</td>\n",
       "      <td>-0.016420</td>\n",
       "      <td>0.020670</td>\n",
       "      <td>0.015714</td>\n",
       "      <td>0.035024</td>\n",
       "      <td>0.056850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4143 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Concept_1  Concept_2  Concept_3  Concept_4  Concept_5  Concept_6   \n",
       "0      0.999849  -0.013726   0.000111   0.009711   0.001872  -0.001286  \\\n",
       "1      0.007587   0.186153   0.008054  -0.299281  -0.056562   0.120939   \n",
       "2      0.016368   0.493583   0.002779  -0.720119  -0.187431   0.271094   \n",
       "3      0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "4      0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "4138   0.006211   0.382960  -0.022118  -0.184783  -0.006990   0.109327   \n",
       "4139   0.006191   0.382418  -0.022140  -0.183659  -0.005000   0.110682   \n",
       "4140   0.006184   0.381924  -0.022072  -0.183385  -0.004627   0.109066   \n",
       "4141   0.006197   0.384153  -0.022106  -0.186593  -0.004606   0.111697   \n",
       "4142   0.006191   0.381609  -0.022023  -0.183640  -0.004929   0.108320   \n",
       "\n",
       "      Concept_7  Concept_8  Concept_9  Concept_10  Concept_11  Concept_12   \n",
       "0      0.001098  -0.002294   0.000646    0.000024    0.000101    0.000257  \\\n",
       "1     -0.023285   0.015316  -0.083345    0.090258    0.287872   -0.010525   \n",
       "2     -0.000146   0.086290   0.074131   -0.047149   -0.164009   -0.112872   \n",
       "3      0.000000   0.000000   0.000000    0.000000    0.000000    0.000000   \n",
       "4      0.000000   0.000000   0.000000    0.000000    0.000000    0.000000   \n",
       "...         ...        ...        ...         ...         ...         ...   \n",
       "4138   0.121565  -0.055678  -0.071760   -0.018371   -0.058241    0.016078   \n",
       "4139   0.120910  -0.055650  -0.074001   -0.018629   -0.058473    0.017759   \n",
       "4140   0.121102  -0.055075  -0.073806   -0.018323   -0.057845    0.018447   \n",
       "4141   0.122782  -0.054554  -0.079079   -0.019551   -0.059865    0.016153   \n",
       "4142   0.120766  -0.053291  -0.074801   -0.018298   -0.057878    0.018845   \n",
       "\n",
       "      Concept_13  Concept_14  Concept_15  Concept_16  Concept_17  Concept_18   \n",
       "0      -0.000028   -0.000156    0.000196   -0.000447   -0.000826   -0.001424  \\\n",
       "1      -0.281764   -0.239309    0.315574    0.059115   -0.006630    0.140469   \n",
       "2       0.063445   -0.042544    0.161560   -0.104036   -0.000914    0.001172   \n",
       "3       0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "4       0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "4138   -0.220787    0.841823    0.129586   -0.017570    0.020281    0.015416   \n",
       "4139   -0.218048    0.842704    0.129941   -0.016752    0.020332    0.014877   \n",
       "4140   -0.218720    0.843001    0.130303   -0.016602    0.020983    0.015862   \n",
       "4141   -0.216957    0.841026    0.125826   -0.017966    0.019164    0.014654   \n",
       "4142   -0.218416    0.843360    0.130181   -0.016420    0.020670    0.015714   \n",
       "\n",
       "      Concept_19  Concept_20  \n",
       "0       0.001743    0.000569  \n",
       "1      -0.657017   -0.260368  \n",
       "2      -0.175392   -0.024019  \n",
       "3       0.000000    0.000000  \n",
       "4       0.000000    0.000000  \n",
       "...          ...         ...  \n",
       "4138    0.035358    0.056915  \n",
       "4139    0.035035    0.056799  \n",
       "4140    0.034881    0.056927  \n",
       "4141    0.039665    0.057861  \n",
       "4142    0.035024    0.056850  \n",
       "\n",
       "[4143 rows x 20 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert the NumPy array to a Pandas DataFrame\n",
    "concept_doc_df = pd.DataFrame(lsa_tfidf, columns=[f\"Concept_{i+1}\" for i in range(lsa_tfidf.shape[1])])\n",
    "\n",
    "# Display the dataframe\n",
    "print(\"Concept-Document Matrix:\")\n",
    "display(concept_doc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.8050)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1\n",
      "     0 347 157\n",
      "     1  45 487\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(lsa_tfidf, df['Label'], test_size=0.25, random_state=42)\n",
    "\n",
    "# run logistic regression model on training\n",
    "logit_reg = LogisticRegression(solver='lbfgs')\n",
    "logit_reg.fit(Xtrain, ytrain)\n",
    "\n",
    "# print confusion matrix and accuracy\n",
    "classificationSummary(ytest, logit_reg.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7562111801242236\n",
      "Recall: 0.9154135338345865\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(ytest, logit_reg.predict(Xtest))\n",
    "\n",
    "recall = recall_score(ytest, logit_reg.predict(Xtest))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.9257)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1\n",
      "     0 467  37\n",
      "     1  40 492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(counts_1, df['Label'], test_size=0.25, random_state=42)\n",
    "\n",
    "# run logistic regression model on training\n",
    "logit_reg = LogisticRegression(solver='lbfgs')\n",
    "logit_reg.fit(Xtrain, ytrain)\n",
    "\n",
    "# print confusion matrix and accuracy\n",
    "classificationSummary(ytest, logit_reg.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9300567107750473\n",
      "Recall: 0.924812030075188\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(ytest, logit_reg.predict(Xtest))\n",
    "\n",
    "recall = recall_score(ytest, logit_reg.predict(Xtest))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.8871)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1\n",
      "     0 433  71\n",
      "     1  46 486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gabri\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(counts_2, df['Label'], test_size=0.25, random_state=42)\n",
    "\n",
    "# run logistic regression model on training\n",
    "logit_reg = LogisticRegression(solver='lbfgs')\n",
    "logit_reg.fit(Xtrain, ytrain)\n",
    "\n",
    "# print confusion matrix and accuracy\n",
    "classificationSummary(ytest, logit_reg.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8725314183123878\n",
      "Recall: 0.9135338345864662\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(ytest, logit_reg.predict(Xtest))\n",
    "\n",
    "recall = recall_score(ytest, logit_reg.predict(Xtest))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.9006)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1\n",
      "     0 428  76\n",
      "     1  27 505\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(tfidf, df['Label'], test_size=0.25, random_state=42)\n",
    "\n",
    "# run logistic regression model on training\n",
    "logit_reg = LogisticRegression(solver='lbfgs')\n",
    "logit_reg.fit(Xtrain, ytrain)\n",
    "\n",
    "# print confusion matrix and accuracy\n",
    "classificationSummary(ytest, logit_reg.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8691910499139415\n",
      "Recall: 0.9492481203007519\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(ytest, logit_reg.predict(Xtest))\n",
    "\n",
    "recall = recall_score(ytest, logit_reg.predict(Xtest))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.9151)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1\n",
      "     0 442  62\n",
      "     1  26 506\n"
     ]
    }
   ],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(tfidf_2, df['Label'], test_size=0.25, random_state=42)\n",
    "\n",
    "# run logistic regression model on training\n",
    "logit_reg = LogisticRegression(solver='lbfgs')\n",
    "logit_reg.fit(Xtrain, ytrain)\n",
    "\n",
    "# print confusion matrix and accuracy\n",
    "classificationSummary(ytest, logit_reg.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8908450704225352\n",
      "Recall: 0.9511278195488722\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(ytest, logit_reg.predict(Xtest))\n",
    "\n",
    "recall = recall_score(ytest, logit_reg.predict(Xtest))\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
